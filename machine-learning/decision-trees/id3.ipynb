{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Условие\n",
    "* Реализирайте алгоритъма за класификационно дърво ID3\n",
    "* Използвайте ***кросвалидация*** за изчисляване на точността на модела върху обучаващото множество. \n",
    "\n",
    "* За избягване на ***overfitting*** използвайте константа **K** -- минимален брой на обучаващи примери в множеството. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo, dotdict\n",
    "from typing import Dict, List, Set, Tuple\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "\n",
    "breast_cancer: dotdict = fetch_ucirepo(id=14)\n",
    "\n",
    "X_df: pd.core.frame.DataFrame = breast_cancer.data.features \n",
    "y_df: pd.core.frame.DataFrame = breast_cancer.data.targets\n",
    "X_df = X_df.fillna(\"?\")\n",
    "\n",
    "values: List[str] = [X_df[name].unique() for idx, name in enumerate(X_df.columns)]\n",
    "\n",
    "X: np.ndarray = X_df.values\n",
    "y: np.ndarray = y_df.values\n",
    "pos_target: str = \"recurrence-events\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Split = Dict[str, \"Data\"]\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, \n",
    "                 X_train_ref,\n",
    "                 y_train_ref,\n",
    "                 values: List[str],\n",
    "                 feature: int = -1,  \n",
    "                 data: List[int] = None):\n",
    "        \n",
    "        self.X_train_ref = X_train_ref\n",
    "        self.y_train_ref = y_train_ref\n",
    "        self.values: List[str] = values \n",
    "        self.num_pos: int = 0\n",
    "        self.feature: int = feature\n",
    "        self.data: List[int] = data if data else []\n",
    "        self._count_pos()\n",
    "\n",
    "    def _count_pos(self) -> None:\n",
    "        for idx in self.data:\n",
    "            if self.y_train_ref[idx] == pos_target:\n",
    "                self.num_pos += 1\n",
    "    @property\n",
    "    def prop_predict(self) -> bool:\n",
    "        return self.prop_pos > 0.5\n",
    "\n",
    "    @property\n",
    "    def prop_pos(self) -> float:\n",
    "        return self.num_pos / len(self.data)\n",
    "    @property\n",
    "    def zero_entropy(self) -> bool:\n",
    "        return self.num_pos == len(self) or self.num_pos == 0 \n",
    "    \n",
    "    @property\n",
    "    def full_entropy(self) -> bool:\n",
    "        return self.num_pos == (len(self) - self.num_pos) \n",
    "    \n",
    "    @property\n",
    "    def entropy(self) -> float:\n",
    "        if self.zero_entropy:\n",
    "            return 0\n",
    "\n",
    "        num_neg: int = (len(self) - self.num_pos) \n",
    "        prop_neg: float = num_neg / len(self)\n",
    "\n",
    "        return -self.num_pos * math.log(self.prop_pos) - num_neg * math.log(prop_neg)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def insert_idx(self, idx: int) -> None:\n",
    "        self.data.append(idx)\n",
    "        if self.y_train_ref[idx] == pos_target:\n",
    "            self.num_pos += 1\n",
    "\n",
    "    def split_on_attribute(self, feature: int) -> Split:\n",
    "        spl: Split = {val: Data(self.X_train_ref, \n",
    "                                self.y_train_ref, \n",
    "                                self.values,\n",
    "                                feature=feature) for val in self.values[feature]}\n",
    "        \n",
    "        for idx in self.data:\n",
    "            val: str = self.X_train_ref[idx][feature]\n",
    "            spl[val].insert_idx(idx)\n",
    "\n",
    "        return spl\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return repr(self.data)\n",
    "\n",
    "# dt = Data(X, y, values, data=list(range(len(X))))\n",
    "# spl: Split = dt.split_on_attribute(0)\n",
    "# # print(dt.data)\n",
    "# # print(dt.num_pos)\n",
    "# # print(dt.prop_pos)\n",
    "# print(dt.entropy)\n",
    "# for key, value in spl.items():\n",
    "#     print(f\"{key}: {value.entropy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    K: int = 30\n",
    "    D: int = 2\n",
    "    def __init__(self, \n",
    "                 attributes: List[bool], \n",
    "                 data: Data=Data, depth=-1) -> None:\n",
    "        \n",
    "        self.data: Data = data\n",
    "        self.children: Dict[str, DecisionTree] = {}\n",
    "        self.attributes: List[bool] = attributes\n",
    "\n",
    "        if depth != -1:\n",
    "            self._build_children_depth(depth)\n",
    "        else:\n",
    "            self._build_children()\n",
    "    \n",
    "    def feature_id(self) -> int:\n",
    "        return self.data.feature\n",
    "\n",
    "    def information_gain(self, spl: Split) -> float:\n",
    "        sum_term: float = sum((len(val) / len(self.data)) * val.entropy for _, val in spl.items())\n",
    "        return self.data.entropy - sum_term\n",
    "\n",
    "    def get_best_split(self) -> Tuple[int, Split]:\n",
    "        best: Split = {}\n",
    "        best_score: float = float(\"-inf\")\n",
    "        feature: int = -1\n",
    "        for i in range(len(self.attributes)):\n",
    "            if not self.attributes[i]:\n",
    "                spl: Split = self.data.split_on_attribute(i)\n",
    "                ig: float = self.information_gain(spl)\n",
    "                if ig > best_score:\n",
    "                    best_score = ig\n",
    "                    best = spl\n",
    "                    feature = i\n",
    "        return feature, best\n",
    "\n",
    "    @property\n",
    "    def leaf(self) -> bool:\n",
    "        return False if self.children else True\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> bool:\n",
    "        if self.leaf:\n",
    "            return self.data.prop_predict\n",
    "        \n",
    "        child = self.children[x[self.attribute]]\n",
    "        if child.leaf and child.data.full_entropy:\n",
    "            return self.data.prop_predict\n",
    "        return child.predict(x)\n",
    "        \n",
    "    @property\n",
    "    def attribute(self) -> str:\n",
    "        return self.data.feature\n",
    "\n",
    "    @property\n",
    "    def all_used(self) -> bool:\n",
    "        return sum(self.attributes) == len(self.attributes)\n",
    "\n",
    "    def _build_children(self) -> None:\n",
    "        if self.data.zero_entropy or self.all_used or len(self.data) < DecisionTree.K + 1:\n",
    "            return\n",
    "\n",
    "        feature, best_split = self.get_best_split()\n",
    "        self.data.feature = feature\n",
    "        self.attributes[feature] = True\n",
    "    \n",
    "        for val, data in best_split.items():\n",
    "            self.children[val] = DecisionTree(self.attributes, data, depth=-1)\n",
    "\n",
    "    def _build_children_depth(self, depth: int) -> None:\n",
    "        if self.data.zero_entropy or self.all_used or depth > DecisionTree.D:\n",
    "            return\n",
    "\n",
    "        feature, best_split = self.get_best_split()\n",
    "        self.data.feature = feature\n",
    "        self.attributes[feature] = True\n",
    "    \n",
    "        for val, data in best_split.items():\n",
    "            self.children[val] = DecisionTree(self.attributes, data, depth=depth + 1)\n",
    "\n",
    "\n",
    "# attributes = [False for _ in range(X.shape[1])]\n",
    "# n = DecisionTree(attributes, Data(X[100:], y[100:], values, data=list(range(len(X[100:])))))\n",
    "\n",
    "# for i in range(len(X)):\n",
    "\n",
    "\n",
    "# print(\"age\", n.information_gain(spl))\n",
    "# print(n.get_best_split())\n",
    "# print(n.children)\n",
    "# print(n.predict(X[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6885467980295565\n",
      "0.6642857142857144\n",
      "0.6921182266009853\n",
      "0.6860837438423645\n",
      "0.6711822660098521\n",
      "0.6852216748768473\n",
      "0.6875615763546798\n",
      "0.6967980295566502\n",
      "0.6779556650246304\n",
      "0.6928571428571428\n",
      "Average:  0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import  train_test_split, KFold\n",
    "\n",
    "\n",
    "def measure_accuracy_num_left_pruning(X, y):\n",
    "    \"\"\"\n",
    "    10-fold cross validation\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        attributes = [False for _ in range(X.shape[1])]\n",
    "        n = DecisionTree(attributes, Data(X_train, y_train, values, data=list(range(len(X_train)))))\n",
    "\n",
    "        mistakes = 0\n",
    "        for i in range(len(X_test)):\n",
    "            temp = y_test[i] == pos_target\n",
    "            if temp != n.predict(X_test[i]):\n",
    "                mistakes += 1\n",
    "        accs.append(1 - mistakes / len(X_test)) \n",
    " \n",
    "    return sum(accs) / len(accs)\n",
    "\n",
    "accs = [measure_accuracy_num_left_pruning(X, y) for _ in range(10)]\n",
    "for elem in accs: \n",
    "    print(elem)\n",
    "\n",
    "print(f\"Average: {sum(accs) / len(accs): .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7023399014778324\n",
      "0.6883004926108374\n",
      "0.6988916256157636\n",
      "0.6923645320197045\n",
      "0.6986453201970444\n",
      "0.7171182266009853\n",
      "0.6915024630541872\n",
      "0.699384236453202\n",
      "0.6955665024630541\n",
      "0.6924876847290641\n",
      "Average:  0.70\n"
     ]
    }
   ],
   "source": [
    "def measure_accuracy_depth_pruning(X, y):\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        attributes = [False for _ in range(X.shape[1])]\n",
    "        n = DecisionTree(attributes, Data(X_train, y_train, \n",
    "                                          values, \n",
    "                                          data=list(range(len(X_train)))), depth=0)\n",
    "\n",
    "        mistakes = 0\n",
    "        for i in range(len(X_test)):\n",
    "            temp = y_test[i] == pos_target\n",
    "            if temp != n.predict(X_test[i]):\n",
    "                mistakes += 1\n",
    "        accs.append(1 - mistakes / len(X_test)) \n",
    "\n",
    "    return sum(accs) / len(accs)\n",
    "\n",
    "\n",
    "accs = [measure_accuracy_depth_pruning(X, y) for _ in range(10)]\n",
    "for elem in accs: \n",
    "    print(elem)\n",
    "\n",
    "print(f\"Average: {sum(accs) / len(accs): .2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Сравнение със ***sklearn***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy criterion, min_samples_split=30, max_depth=None\n",
      "Average:  0.73\n",
      "entropy criterion, min_samples_split=2, max_depth=2\n",
      "Average:  0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "X_encoded = pd.DataFrame(encoder.fit_transform(X_df), columns=X_df.columns)\n",
    "y_encoded = pd.DataFrame(encoder.fit_transform(y_df), columns=y_df.columns)\n",
    "\n",
    "print(f\"entropy criterion, min_samples_split={DecisionTree.K}, max_depth=None\")\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", \n",
    "                                  min_samples_split=DecisionTree.K)\n",
    "accs = cross_val_score(clf, X_encoded, y_encoded, cv=10)\n",
    "print(f\"Average: {accs.mean(): .2f}\")\n",
    "\n",
    "\n",
    "print(f\"entropy criterion, min_samples_split=2, max_depth={DecisionTree.D}\")\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", \n",
    "                                  max_depth=DecisionTree.D)\n",
    "accs = cross_val_score(clf, X_encoded, y_encoded, cv=10)\n",
    "print(f\"Average: {accs.mean(): .2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Samples B times with replacement and build a forest of B trees. \n",
    "    K is the number of features to be used to build the tree. \n",
    "    Picks random subset of K features for each of the trees in the forest.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train: np.ndarray, y_train, B: int = 100, K: int = -1) -> None:\n",
    "        indices: np.ndarray = [np.random.choice(len(X_train), \n",
    "                                                size=len(X_train), \n",
    "                                                replace=True) for _ in range(B)]\n",
    "        if K == -1:\n",
    "            K = int(math.sqrt(X.shape[1]))\n",
    "        \n",
    "        features: np.ndarray = np.arange(X.shape[1])\n",
    "        columns: List[np.ndarray] = [np.sort(np.random.choice(features, \n",
    "                                                              size=K, \n",
    "                                                              replace=False)) for _ in range(B)]\n",
    " \n",
    "        X_trains: List[np.ndarray] = [X_train[indx] \n",
    "                                      for indx, X \n",
    "                                      in zip(indices, X)]\n",
    "\n",
    "        y_trains: List[np.ndarray] = [y_train[indx] \n",
    "                                      for indx, y \n",
    "                                      in zip(indices, y)]\n",
    " \n",
    "\n",
    "        values: List[List[str]] = [np.unique(X[:, i]) for i in range(X.shape[1])]\n",
    "        \n",
    "        \n",
    "        self.models: List[DecisionTree] = [DecisionTree([i not in columns[k] for i in range(X_train.shape[1])], # forbidden use of attributes not chosen\n",
    "                                                         Data(X, y, values, data=list(range(len(X)))), depth=0) \n",
    "                                                         for k, (X, y) in enumerate(zip(X_trains, y_trains))] # Build forest\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> bool:\n",
    "        votes_pos = sum(model.predict(X) for model in self.models)\n",
    "        return votes_pos / len(self.models) > 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators: int = 64\n",
    "K: int = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "measure_accuracy_rf() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [144], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m         accs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mistakes \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_test)) \n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(accs) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(accs)\n\u001b[0;32m---> 25\u001b[0m accs \u001b[38;5;241m=\u001b[39m [measure_accuracy_rf(X, y, k) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m accs: \n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(elem)\n",
      "Cell \u001b[0;32mIn [144], line 25\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m         accs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mistakes \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_test)) \n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(accs) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(accs)\n\u001b[0;32m---> 25\u001b[0m accs \u001b[38;5;241m=\u001b[39m [\u001b[43mmeasure_accuracy_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m accs: \n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(elem)\n",
      "\u001b[0;31mTypeError\u001b[0m: measure_accuracy_rf() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "def measure_accuracy_rf(X, y):\n",
    "    \"\"\"\n",
    "    10-fold cross validation\n",
    "    \"\"\"\n",
    "    global n_estimators, K\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        rf: RandomForest = RandomForest(X, y, n_estimators, K)\n",
    "\n",
    "        mistakes = 0\n",
    "        for i in range(len(X_test)):\n",
    "            temp = y_test[i] == pos_target\n",
    "            if temp != rf.predict(X_test[i]):\n",
    "                mistakes += 1\n",
    "        accs.append(1 - mistakes / len(X_test)) \n",
    " \n",
    "    return sum(accs) / len(accs)\n",
    "\n",
    "accs = [measure_accuracy_rf(X, y) for _ in range(10)]\n",
    "\n",
    "for elem in accs: \n",
    "    print(elem)\n",
    "\n",
    "print(f\"Average: {sum(accs) / len(accs): .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.7445812807881773\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                            criterion=\"entropy\",\n",
    "                            max_features=k, \n",
    "                            max_depth=DecisionTree.D)\n",
    "accs = cross_val_score(clf, X_encoded, y_encoded['Class'], cv=10)\n",
    "\n",
    "print(f\"Average accuracy: {accs.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
