{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Условие\n",
    "* Реализирайте алгоритъма за класификационно дърво ID3\n",
    "* Използвайте ***кросвалидация*** за изчисляване на точността на модела върху обучаващото множество. \n",
    "\n",
    "* За избягване на ***overfitting*** използвайте константа **K** -- минимален брой на обучаващи примери в множеството. \n",
    "\n",
    "? друг подход за избягване на ***overfittting*** + сравняване на резултата?  \n",
    "? bonus: ***Random Forest*** ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo, dotdict\n",
    "from typing import Dict, List, Set, Tuple\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "\n",
    "breast_cancer: dotdict = fetch_ucirepo(id=14)\n",
    "\n",
    "X: pd.core.frame.DataFrame = breast_cancer.data.features \n",
    "y: pd.core.frame.DataFrame = breast_cancer.data.targets\n",
    "\n",
    "values: List[str] = [X[name].unique() for idx, name in enumerate(X.columns)]\n",
    "\n",
    "X: np.ndarray = X.values\n",
    "y: np.ndarray = y.values\n",
    "pos_target: str = \"recurrence-events\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Split = Dict[str, \"Data\"]\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, \n",
    "                 X_train_ref,\n",
    "                 y_train_ref,\n",
    "                 feature: int = -1,  \n",
    "                 data: List[int] = None):\n",
    "        \n",
    "        self.X_train_ref = X_train_ref\n",
    "        self.y_train_ref = y_train_ref \n",
    "        self.num_pos: int = 0\n",
    "        self.feature: int = feature\n",
    "        self.data: List[int] = data if data else []\n",
    "        self._count_pos()\n",
    "\n",
    "    def _count_pos(self) -> None:\n",
    "        for idx in self.data:\n",
    "            if self.y_train_ref[idx] == pos_target:\n",
    "                self.num_pos += 1\n",
    "    @property\n",
    "    def prop_predict(self) -> bool:\n",
    "        return self.prop_pos > 0.5\n",
    "\n",
    "    @property\n",
    "    def prop_pos(self) -> float:\n",
    "        return self.num_pos / len(self.data)\n",
    "    @property\n",
    "    def zero_entropy(self) -> bool:\n",
    "        return self.num_pos == len(self) or self.num_pos == 0 \n",
    "    \n",
    "    @property\n",
    "    def full_entropy(self) -> bool:\n",
    "        return self.num_pos == (len(self) - self.num_pos) \n",
    "    \n",
    "    @property\n",
    "    def entropy(self) -> float:\n",
    "        if self.zero_entropy:\n",
    "            return 0\n",
    "\n",
    "        num_neg: int = (len(self) - self.num_pos) \n",
    "        prop_neg: float = num_neg / len(self)\n",
    "\n",
    "        return -self.num_pos * math.log(self.prop_pos) - num_neg * math.log(prop_neg)\n",
    "\n",
    "    @property\n",
    "    def gini(self) -> float:\n",
    "        return 2 * self.pos_prop * (1 - self.pos_prop)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def insert_idx(self, idx: int) -> None:\n",
    "        self.data.append(idx)\n",
    "        if self.y_train_ref[idx] == pos_target:\n",
    "            self.num_pos += 1\n",
    "\n",
    "    def split_on_attribute(self, feature: int) -> Split:\n",
    "        spl: Split = {val: Data(self.X_train_ref, \n",
    "                                self.y_train_ref, \n",
    "                                feature=feature) for val in values[feature]}\n",
    "        \n",
    "        for idx in self.data:\n",
    "            val: str = self.X_train_ref[idx][feature]\n",
    "            spl[val].insert_idx(idx)\n",
    "\n",
    "        return spl\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return repr(self.data)\n",
    "\n",
    "dt = Data(X, y, data=list(range(len(X))))\n",
    "# spl: Split = dt.split_on_attribute(0)\n",
    "# print(dt.data)\n",
    "# print(dt.num_pos)\n",
    "# print(dt.prop_pos)\n",
    "# print(dt.entropy)\n",
    "# for key, value in spl.items():\n",
    "#     print(f\"{key}: {value.entropy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    K: int = 20\n",
    "    D: int = 2\n",
    "    def __init__(self, \n",
    "                 attributes: List[bool], \n",
    "                 data: Data=Data, depth=-1) -> None:\n",
    "        self.data: Data = data\n",
    "        self.children: Dict[str, DecisionTree] = {}\n",
    "        self.attributes: List[bool] = attributes\n",
    "\n",
    "        if depth != -1:\n",
    "            self._build_children_depth(depth)\n",
    "        else:\n",
    "            self._build_children()\n",
    "    \n",
    "    def feature_id(self) -> int:\n",
    "        return self.data.feature\n",
    "\n",
    "    def information_gain(self, spl: Split) -> float:\n",
    "        sum_term: float = sum((len(val) / len(self.data)) * val.entropy for _, val in spl.items())\n",
    "        return self.data.entropy - sum_term\n",
    "\n",
    "    def get_best_split(self) -> Tuple[int, Split]:\n",
    "        best: Split = {}\n",
    "        best_score: float = float(\"-inf\")\n",
    "        feature: int = -1\n",
    "        for i in range(len(self.attributes)):\n",
    "            if not self.attributes[i]:\n",
    "                spl: Split = self.data.split_on_attribute(i)\n",
    "                ig: float = self.information_gain(spl)\n",
    "                if ig > best_score:\n",
    "                    best_score = ig\n",
    "                    best = spl\n",
    "                    feature = i\n",
    "        return feature, best\n",
    "\n",
    "    @property\n",
    "    def leaf(self) -> bool:\n",
    "        return False if self.children else True\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> bool:\n",
    "        if self.leaf:\n",
    "            return self.data.prop_predict\n",
    "        \n",
    "        for val, child in self.children.items():\n",
    "            if x[self.attribute] == val:\n",
    "                if child.leaf and child.data.full_entropy:\n",
    "                    return self.data.prop_predict\n",
    "                return child.predict(x)\n",
    "\n",
    "    @property\n",
    "    def attribute(self) -> str:\n",
    "        return self.data.feature\n",
    "\n",
    "    @property\n",
    "    def all_used(self) -> bool:\n",
    "        return sum(self.attributes) == len(self.attributes)\n",
    "\n",
    "    def _build_children(self) -> None:\n",
    "        if self.data.zero_entropy or self.all_used or len(self.data) < DecisionTree.K:\n",
    "            return\n",
    "\n",
    "        feature, best_split = self.get_best_split()\n",
    "        self.data.feature = feature\n",
    "        self.attributes[feature] = True\n",
    "    \n",
    "        for val, data in best_split.items():\n",
    "            self.children[val] = DecisionTree(self.attributes, data, depth=-1)\n",
    "\n",
    "    def _build_children_depth(self, depth: int) -> None:\n",
    "        if self.data.zero_entropy or self.all_used or depth > DecisionTree.D:\n",
    "            return\n",
    "\n",
    "        feature, best_split = self.get_best_split()\n",
    "        self.data.feature = feature\n",
    "        self.attributes[feature] = True\n",
    "    \n",
    "        for val, data in best_split.items():\n",
    "            self.children[val] = DecisionTree(self.attributes, data, depth=depth + 1)\n",
    "\n",
    "\n",
    "attributes = [False for _ in range(X.shape[1])]\n",
    "n = DecisionTree(attributes, Data(X[100:], y[100:], data=list(range(len(X[100:])))))\n",
    "\n",
    "# for i in range(len(X)):\n",
    "\n",
    "\n",
    "# print(\"age\", n.information_gain(spl))\n",
    "# print(n.get_best_split())\n",
    "# print(n.children)\n",
    "# print(n.predict(X[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6668719211822661\n",
      "0.6578817733990148\n",
      "0.6467980295566502\n",
      "0.6710591133004926\n",
      "0.668472906403941\n",
      "0.6466748768472905\n",
      "0.6535714285714286\n",
      "0.6610837438423646\n",
      "0.670935960591133\n",
      "0.6854679802955664\n",
      "Average:  0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import  train_test_split, KFold\n",
    "\n",
    "\n",
    "def measure_accuracy_num_left_pruning(X, y):\n",
    "    \"\"\"\n",
    "    10-fold cross validation\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        attributes = [False for _ in range(X.shape[1])]\n",
    "        n = DecisionTree(attributes, Data(X_train, y_train, data=list(range(len(X_train)))))\n",
    "\n",
    "        mistakes = 0\n",
    "        for i in range(len(X_test)):\n",
    "            temp = y_test[i] == pos_target\n",
    "            if temp != n.predict(X_test[i]):\n",
    "                mistakes += 1\n",
    "        accs.append(1 - mistakes / len(X_test)) \n",
    " \n",
    "    return sum(accs) / len(accs)\n",
    "\n",
    "accs = [measure_accuracy_num_left_pruning(X, y) for _ in range(10)]\n",
    "for elem in accs: \n",
    "    print(elem)\n",
    "\n",
    "print(f\"Average: {sum(accs) / len(accs): .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6848522167487684\n",
      "0.6955665024630543\n",
      "0.705665024630542\n",
      "0.6887931034482759\n",
      "0.6788177339901479\n",
      "0.6784482758620691\n",
      "0.7030788177339902\n",
      "0.6992610837438423\n",
      "0.6782019704433498\n",
      "0.6887931034482758\n",
      "Average:  0.69\n"
     ]
    }
   ],
   "source": [
    "def measure_accuracy_depth_pruning(X, y):\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        attributes = [False for _ in range(X.shape[1])]\n",
    "        n = DecisionTree(attributes, Data(X_train, y_train, data=list(range(len(X_train)))), depth=0)\n",
    "\n",
    "        mistakes = 0\n",
    "        for i in range(len(X_test)):\n",
    "            temp = y_test[i] == pos_target\n",
    "            if temp != n.predict(X_test[i]):\n",
    "                mistakes += 1\n",
    "        accs.append(1 - mistakes / len(X_test)) \n",
    "\n",
    "    return sum(accs) / len(accs)\n",
    "\n",
    "\n",
    "accs = [measure_accuracy_depth_pruning(X, y) for _ in range(10)]\n",
    "for elem in accs: \n",
    "    print(elem)\n",
    "\n",
    "print(f\"Average: {sum(accs) / len(accs): .2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Сравнение със ***sklearn***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy criterion, min_samples_split=2, max_depth=None\n",
      "Average:  0.66\n",
      "gini criterion, min_samples_split=2, max_depth=None\n",
      "Average: 0.687807881773399\n",
      "entropy criterion, min_samples_split=20, max_depth=None\n",
      "Average: 0.6732758620689655\n",
      "entropy criterion, min_samples_split = 2, max_depth=2\n",
      "Average: 0.7088669950738917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "breast_cancer: dotdict = fetch_ucirepo(id=14)\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "X: pd.core.frame.DataFrame = breast_cancer.data.features \n",
    "y: pd.core.frame.DataFrame = breast_cancer.data.targets\n",
    "\n",
    "X_encoded = pd.DataFrame(encoder.fit_transform(X), columns=X.columns)\n",
    "y_encoded = pd.DataFrame(encoder.fit_transform(y), columns=y.columns)\n",
    "\n",
    "print(f\"entropy criterion, min_samples_split=2, max_depth=None\")\n",
    "clf = tree.DecisionTreeClassifier(random_state=0, \n",
    "                                  criterion=\"entropy\")\n",
    "accs = cross_val_score(clf, X_encoded, y_encoded, cv=10)\n",
    "print(f\"Average: {accs.mean(): .2f}\")\n",
    "\n",
    "print(f\"gini criterion, min_samples_split=2, max_depth=None\")\n",
    "clf = tree.DecisionTreeClassifier(random_state=0, \n",
    "                                  criterion=\"gini\")\n",
    "accs = cross_val_score(clf, X_encoded, y_encoded, cv=10)\n",
    "print(f\"Average: {accs.mean(): .2f}\")\n",
    "\n",
    "print(f\"entropy criterion, min_samples_split={DecisionTree.K}, max_depth=None\")\n",
    "clf = tree.DecisionTreeClassifier(random_state=0, \n",
    "                                  criterion=\"entropy\", \n",
    "                                  min_samples_split=DecisionTree.K)\n",
    "accs = cross_val_score(clf, X_encoded, y_encoded, cv=10)\n",
    "print(f\"Average: {accs.mean(): .2f}\")\n",
    "\n",
    "\n",
    "print(f\"entropy criterion, min_samples_split = 2, max_depth={DecisionTree.D}\")\n",
    "clf = tree.DecisionTreeClassifier(random_state=0, \n",
    "                                  criterion=\"entropy\", \n",
    "                                  max_depth=DecisionTree.D)\n",
    "accs = cross_val_score(clf, X_encoded, y_encoded, cv=10)\n",
    "print(f\"Average: {accs.mean(): .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
