# Notes

### Curse of dimentionality

As the number of features/dimentions grows, the amount of data we need to make accurate generalizations grows exponentially.

However, we rarely have enough data and hence the models are more prone to overfitting.

With a fixed number of training samples, the average predictivite power of a classifier or regressor first increases as the number of dimentions/features used is increased but beyond a certain dimentionality it starts deteriorating instead of imporving steadily.
